---
title: "portalcasting Codebase"
output: rmarkdown::html_vignette
author: "Juniper L. Simonis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
vignette: >
  %\VignetteIndexEntry{portalcasting_codebase}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include=FALSE}
library(portalcasting)
vers <- packageVersion("portalcasting")
today <- Sys.Date()
```

This vignette outlines the codebase and functionality of the **portalcasting** package (v`r vers`), which underlies the automated iterative forecasting within the [Portal Predictions production pipeline](https://github.com/weecology/portalPredictions). **portalcasting** has utilities for setting up local versions of the pipeline for developing and testing new models, which are covered in detail in other vignettes.

## Installation

To install the most recent version of **portalcasting** from GitHub:

```{r, eval=FALSE}
install.packages("devtools")
devtools::install_github("weecology/portalcasting")
```

## Directory Structure

The package uses a directory tree with two levels to organize the project:

* `main`: project folder encompassing all subfolders
* `subs`: specific subfolders that organize the project files
 
structured as

```
main
│
└──resources
│   <stable version of resources used to populate other folders>
└──models
│   <model scripts>
└──data
│   <data used for a specific run of models>
└──casts
│   <previous and current model casts>
└──fits
│   <previous and current model fits>
└──config.yaml
```

To group the project subfolders into a multi-leveled folder, simply add structure to the `main` input, such as `main = "~/project_folder"`.

## Instantiating a Directory

Setting up a fully functional directory for a production or sandbox pipeline consists of two steps: creating (instantiating folders that are missing) and filling (adding files to the folders). These steps can be executed separately or in combination via a general `setup_dir` function or via specialized versions of `setup_dir`: `setup_sandbox` (for creating a pipeline with defaults to facilitate sandboxing) and `setup_production` (for creating a production pipeline). These functions are general and flexible, allowing for customization through a variety of arguments, but are designed to work well under default settings.

### Creating 

The directory is established using `create_dir`, which takes `main` as an argument and in sequence creates each of the levels' folders if they do not already exist. This occurs through calls to `create_main` and `create_subs`. A typical user is likely to want to change the `main` input (to locate the forecasting directory where they would like it), but general users should not alter the `subs` structure, and so that option is not available through the `create` functions.

`create_dir` also initializes the `dir_config.yaml` file, which is held within `main` and contains metadata about the directory setting up process.

### Filling 

The directory is filled (loaded with files for forecasting) using a series of subdirectory-specific functions that are combined in the overall `fill_dir` function:

* `fill_raw` downloads each of the raw components for the directory, which presently include the source data and previous forecasts' archive. Upon completion of the downloads, `fill_raw` updates `dir_config.yaml` with download versions.
* `fill_casts` moves the existing model output files from the `raw` subdirectory to the `casts` subdirectory.
* `fill_models` writes the model scripts into the `models` subdirectory.
* `fill_data` prepares the forecasting data files from the raw downloaded data files and moves them into the `data` subdirectory.
  * `prep_moons` prepares and formats the temporal (lunar) data from the raw data.
  * `prep_rodents` prepares multiple structures of the rodents data for analyses from the raw data.
  * `prep_covariates` downloads and forecasts covariates data.
  * `prep_metadata` creates and saves out a YAML metadata list for the forecasting. 

Each of these components can be run individually, as well. In particular, `fill_data` is used to set up the complete set of data for a given model run, and to reset the data to the most up-to-date version after model completion.

## Running models

Models are run using a function pipeline similar to the creation and filling function pipelines, with flexible controls through a variety of arguments, but robust operation under default settings. 

* `portalcast` is the overarching function that controls casting of the Portal data
  * `verify_models` ensures that all of the models requested have scripts present in the `models` subdirectory.
  * `verify_raw_data` ensures that the base Portal data are present in the `raw` subdirectory
  * `extract_min_lag` determines the minimum non-0 lag used with covariate data across the models, for input into `prep_data`
  * `read_moons` brings the lunar data in to the function and `last_newmoon` determines the most recently passed newmoon, which is used to set the forecast origin (`end_moons`, note that here the plural in `end_moons` indicates that multiple forecast origins can be input to `portalcast`) if it wasn't set by the user.
  * For each `end_moons` value, `portalcast` runs:
    * `fill_data` which ensures that the data files in the `data` subdirectory are up-to-date for the specifics requested. 
    * `cast` runs ("casts") each of the requested models for the data
      * `clear_tmp` clears the files in the `tmp` folder.
      * `models_to_cast` collects the file paths to the scripts in the `models` subdirectory, which are then run using `source`.
      * `clear_tmp` clears the files in the `tmp` folder.



## Utilities

To facilitate tidy and easy-to-follow code, we introduce a few important utility functions, which are put to use throughout the codebase.

### File paths

Given the reliance of files located in a variety of subdirectories that could be operating on multiple platforms, we constructed some simple functions for managing and point to particular folders or files.

* `file_ext` determines the file extension, based on the separating character (`sep_char`), which facilitates use with generalized URL APIs
* `path_no_ext` provides extension-removing services.
* `main_path`, `sub_path`, and `file_path` all provide normalized paths to the named components.

### Data IO

**portalcasting** has a generalized `read_data` function that allows for toggling among `read_rodents`, `read_rodents_table`, `read_covariates`, `read_covariate_casts`, `read_moons`, and `read_metadata`, which each have specific loading procedures in place and, in the case that the requested file does not exist, an attempt is made (through the corresponding `prep_` function) to create the data file. Similar to the `read_data` functions, `read_casts` provides a simple user interface for reading the cast files into the R session.

For saving out, `write_data` provides a simple means for interfacing with potentially pre-existing data files, with logical inputs for saving generally and overwriting a pre-existing file specifically, and flexible file naming. The type of data saved out is currently restricted to `.csv` and `.yaml`, which is extracted from the filename given. 

The directory configuration file is a special file, and has its own IO functions separate from the rest: `write_directory_config` creates the file (from within `create_dir`, `update_directory_config` adds downloads information (from inside `fill_raw`) `read_directory_config` brings the information from the file into the R session.

### Argument checking

**portalcasting** provides tidy and toggle-able argument checking via `check_arg`, `check_args`, and `check_arg_list`. They check the validity of inputted values for arguments, variably (depending on the argument) including the class, length, and values. It extracts the boolean checking code that would normally be within each function, placing it instead within `check_arg` which is called for each argument by `check_args`.

A key aspect to the **portalcasting** pipeline's ability to connect pieces is tied directly to argument checking: argument names should not be re-used in different functions unless they are for the same type of object with the same value options.

Some base **portalcasting** utilities cannot use `check_args` however, and so have their own specific internal checking. For example, `list_depth`'s recursive nature prevents its use, and `check_arg` cannot use `check_args` itself. 

`check_args` provides important backstopping and error checking of a standard **portalcasting** production pipeline. However, it is likely that a user in development spaces (a.k.a. sandboxes) would like to deviate from the standard behavior to leverage, for example, the ability to construct novel rodent data sets. To that end, we include an argument `arg_checks` that is in nearly every function throughout the codebase, which flips a switch and turns off the vast majority of the **portalcasting**-generated errors. In this space, the user will be able to create a much broader array of directory components and actions, but will likely be first informed of errors through more base (and thus obtuse) functions. To that end, in default sandboxing mode, we set `arg_checks = FALSE` (minimal checking from **portalcasting**) and `verbose = TRUE` (maximum feedback from underlying functions).

### `messageq`

`messageq` provides a simple wrapper on `message` that also has a logical input for quieting. This helps switch messaging off as desired while localizing the actual boolean operator code to one spot.
